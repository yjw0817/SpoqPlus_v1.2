#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SpoqPlus Advanced Face Recognition Service
OpenCV + MediaPipe ê¸°ë°˜ ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥:
- ê¸°ë³¸ ì–¼êµ´ ê²€ì¶œ (OpenCV Haar Cascade)
- ëˆˆ ê¹œë¹¡ì„ ê°ì§€ (MediaPipe + EAR)
- ì•ˆê²½ ê°ì§€ ë° ê´€ë¦¬ (MediaPipe ëœë“œë§ˆí¬)
- ìŠ¤í‘¸í•‘ ë°©ì§€ (ì›€ì§ì„, í…ìŠ¤ì²˜ ë¶„ì„)
"""

import cv2
import numpy as np
import json
import mysql.connector
from flask import Flask, request, jsonify
from flask_cors import CORS
import time
import logging
import sys
import os
from dotenv import load_dotenv
import math
import random
from typing import Dict, List, Tuple, Optional
from werkzeug.utils import secure_filename
from PIL import Image
import pymysql

# MediaPipe ì„í¬íŠ¸ (ì„¤ì¹˜ë˜ì–´ ìˆì„ ë•Œë§Œ)
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("âœ… MediaPipe ì‚¬ìš© ê°€ëŠ¥ - ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ í™œì„±í™”")
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("âš ï¸  MediaPipe ì—†ìŒ - ê¸°ë³¸ ê¸°ëŠ¥ë§Œ ì‚¬ìš©")

# .env íŒŒì¼ ë¡œë“œ (config.env â†’ .envë¡œ ë³µì‚¬ í•„ìš”)
load_dotenv('config.env')

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def euclidean_distance(point1: Tuple[float, float], point2: Tuple[float, float]) -> float:
    """ìœ í´ë¦¬ë””ì•ˆ ê±°ë¦¬ ê³„ì‚° í•¨ìˆ˜ (scipy ëŒ€ì²´)"""
    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)

class AdvancedFaceRecognizer:
    """ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì–¼êµ´ ì¸ì‹ê¸°"""
    
    def __init__(self):
        logger.info("ğŸš€ Advanced Face Recognizer ì´ˆê¸°í™”")
        
        # OpenCV ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        self.face_cascade = cv2.CascadeClassifier(cascade_path)
        
        if self.face_cascade.empty():
            logger.error("âŒ OpenCV ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì‹¤íŒ¨")
            raise Exception("OpenCV ì–¼êµ´ ê²€ì¶œê¸°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        logger.info("âœ… OpenCV ì–¼êµ´ ê²€ì¶œê¸° ë¡œë“œ ì™„ë£Œ")
        
        # MediaPipe ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
        self.mediapipe_enabled = MEDIAPIPE_AVAILABLE
        if self.mediapipe_enabled:
            try:
                mp_face_mesh = mp.solutions.face_mesh
                mp_face_detection = mp.solutions.face_detection
                
                self.face_mesh = mp_face_mesh.FaceMesh(
                    max_num_faces=2,
                    refine_landmarks=True,
                    min_detection_confidence=0.5,
                    min_tracking_confidence=0.5
                )
                self.face_detection = mp_face_detection.FaceDetection(
                    model_selection=0,
                    min_detection_confidence=0.5
                )
                
                # ë³´ì•ˆ ê¸°ëŠ¥ ì„¤ì •
                self.EAR_THRESHOLD = 0.25
                self.BLINK_FRAMES = 3
                self.blink_counter = 0
                self.total_blinks = 0
                self.movement_history = []
                self.texture_scores = []
                
                print("ğŸ¯ MediaPipe ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ ì´ˆê¸°í™” ì™„ë£Œ")
                
            except Exception as e:
                print(f"âš ï¸  MediaPipe ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.mediapipe_enabled = False
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ë¡œë“œ
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'user': os.getenv('DB_USER', 'root'),
            'password': os.getenv('DB_PASSWORD', ''),
            'database': os.getenv('DB_NAME', 'spoqplus'),
            'charset': 'utf8mb4',
            'collation': 'utf8mb4_general_ci',
            'port': int(os.getenv('DB_PORT', '3306')),
            'autocommit': True,
            'use_unicode': True
        }
        
        logger.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •: {self.db_config['host']}:{self.db_config['port']}/{self.db_config['database']}")
        
        # í†µê³„
        self.stats = {
            'start_time': time.time(),
            'total_requests': 0,
            'face_detections': 0,
            'successful_recognitions': 0
        }
        
        # í…ŒìŠ¤íŠ¸ ì–¼êµ´ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë¡œë“œ)
        self.test_faces = {
            999: "í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì",  # í…ŒìŠ¤íŠ¸ìš© íšŒì›
            1: "ê´€ë¦¬ì",
            2: "ì‚¬ìš©ì1"
        }
        
        logger.info("âœ… Advanced Face Recognizer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def detect_faces_basic(self, image: np.ndarray) -> Dict:
        """ê¸°ë³¸ OpenCV ì–¼êµ´ ê²€ì¶œ"""
        try:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¡œ ê²€ì¶œ ì‹œë„
            faces = self.face_cascade.detectMultiScale(
                gray, 
                scaleFactor=1.1, 
                minNeighbors=5, 
                minSize=(30, 30)
            )
            
            face_list = []
            for (x, y, w, h) in faces:
                face_list.append({
                    'x': int(x),
                    'y': int(y), 
                    'width': int(w),
                    'height': int(h),
                    'confidence': 0.8  # ê¸°ë³¸ ì‹ ë¢°ë„
                })
            
            return {
                'success': True,
                'face_count': len(faces),
                'faces': face_list,
                'method': 'OpenCV_Haar_Cascade',
                'image_size': {
                    'width': image.shape[1],
                    'height': image.shape[0]
                }
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'face_count': 0
            }
    
    def calculate_ear(self, eye_landmarks: List[Tuple[float, float]]) -> float:
        """Eye Aspect Ratio (EAR) ê³„ì‚°"""
        try:
            if len(eye_landmarks) < 6:
                return 0.4
                
            # ìˆ˜ì§ ê±°ë¦¬ ê³„ì‚°
            A = euclidean_distance(eye_landmarks[1], eye_landmarks[5])
            B = euclidean_distance(eye_landmarks[2], eye_landmarks[4])
            
            # ìˆ˜í‰ ê±°ë¦¬ ê³„ì‚°  
            C = euclidean_distance(eye_landmarks[0], eye_landmarks[3])
            
            # EAR ê³„ì‚°
            if C > 0:
                ear = (A + B) / (2.0 * C)
                return ear
            return 0.4
        except:
            return 0.4
    
    def detect_blink(self, landmarks: np.ndarray, image_shape: Tuple[int, int]) -> Dict:
        """ëˆˆ ê¹œë¹¡ì„ ê°ì§€ (MediaPipe ê¸°ë°˜)"""
        try:
            height, width = image_shape[:2]
            
            # ì™¼ìª½ ëˆˆ ì¢Œí‘œ (MediaPipe ëœë“œë§ˆí¬ ì¸ë±ìŠ¤)
            left_eye_indices = [33, 160, 158, 133, 153, 144]
            left_eye_coords = []
            for idx in left_eye_indices:
                x = landmarks[idx].x * width
                y = landmarks[idx].y * height
                left_eye_coords.append((x, y))
            
            # ì˜¤ë¥¸ìª½ ëˆˆ ì¢Œí‘œ
            right_eye_indices = [362, 385, 387, 263, 373, 380]
            right_eye_coords = []
            for idx in right_eye_indices:
                x = landmarks[idx].x * width
                y = landmarks[idx].y * height
                right_eye_coords.append((x, y))
            
            # EAR ê³„ì‚°
            left_ear = self.calculate_ear(left_eye_coords)
            right_ear = self.calculate_ear(right_eye_coords)
            ear = (left_ear + right_ear) / 2.0
            
            # ê¹œë¹¡ì„ ê°ì§€
            blink_detected = False
            if ear < self.EAR_THRESHOLD:
                self.blink_counter += 1
            else:
                if self.blink_counter >= self.BLINK_FRAMES:
                    self.total_blinks += 1
                    blink_detected = True
                self.blink_counter = 0
            
            return {
                'ear': round(ear, 3),
                'blink_detected': blink_detected,
                'total_blinks': self.total_blinks,
                'is_alive': ear > 0.15,
                'eye_status': 'closed' if ear < self.EAR_THRESHOLD else 'open'
            }
            
        except Exception as e:
            return {
                'ear': 0.0,
                'blink_detected': False,
                'total_blinks': 0,
                'is_alive': False,
                'error': str(e)
            }
    
    def detect_glasses(self, landmarks: np.ndarray, image: np.ndarray) -> Dict:
        """ì•ˆê²½ ì°©ìš© ê°ì§€ (MediaPipe ê¸°ë°˜)"""
        try:
            height, width = image.shape[:2]
            
            # ì½”ë‹¤ë¦¬ ì˜ì—­ ë¶„ì„
            nose_bridge_indices = [6, 168, 8, 9, 10, 151]
            nose_points = []
            for idx in nose_bridge_indices:
                x = int(landmarks[idx].x * width)
                y = int(landmarks[idx].y * height)
                nose_points.append((x, y))
            
            # ì•ˆê²½ ê°ì§€ ì§€í‘œë“¤
            indicators = 0
            
            # 1. ì½”ë‹¤ë¦¬ ë¶€ë¶„ í”½ì…€ ê°•ë„ ë³€í™”
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            
            # ì½”ë‹¤ë¦¬ ì˜ì—­ ROI
            if len(nose_points) >= 4:
                x_coords = [p[0] for p in nose_points]
                y_coords = [p[1] for p in nose_points] 
                
                x_min, x_max = max(0, min(x_coords) - 5), min(width, max(x_coords) + 5)
                y_min, y_max = max(0, min(y_coords) - 10), min(height, max(y_coords) + 10)
                
                nose_roi = gray[y_min:y_max, x_min:x_max]
                
                if nose_roi.size > 0:
                    # í”½ì…€ ê°•ë„ ë¶„ì‚° (ì•ˆê²½ë‹¤ë¦¬ê°€ ìˆìœ¼ë©´ ë¶„ì‚°ì´ í¼)
                    intensity_var = np.var(nose_roi)
                    if intensity_var > 200:
                        indicators += 1
            
            # 2. ëˆˆì¹ ìœ„ìª½ ìˆ˜í‰ì„  ê²€ì¶œ (ì•ˆê²½ í”„ë ˆì„)
            brow_indices = [70, 63, 105, 66, 107]  # ëˆˆì¹ ëœë“œë§ˆí¬
            brow_y_avg = np.mean([landmarks[idx].y * height for idx in brow_indices])
            
            # ëˆˆì¹ ìœ„ìª½ ì˜ì—­ì—ì„œ ì—£ì§€ ê²€ì¶œ
            brow_region = gray[max(0, int(brow_y_avg) - 20):int(brow_y_avg), :]
            if brow_region.size > 0:
                edges = cv2.Canny(brow_region, 50, 150)
                lines = cv2.HoughLinesP(edges, 1, np.pi/180, threshold=20, minLineLength=15, maxLineGap=3)
                
                if lines is not None and len(lines) > 2:
                    indicators += 1
            
            # 3. ë°˜ì‚¬ê´‘ íŒ¨í„´ (ë Œì¦ˆ ë°˜ì‚¬)
            left_eye_center = landmarks[468]  # ì™¼ìª½ ëˆˆ ì¤‘ì‹¬
            right_eye_center = landmarks[473]  # ì˜¤ë¥¸ìª½ ëˆˆ ì¤‘ì‹¬
            
            reflection_count = 0
            for eye_center in [left_eye_center, right_eye_center]:
                eye_x = int(eye_center.x * width)
                eye_y = int(eye_center.y * height)
                
                # ëˆˆ ì¤‘ì‹¬ ì£¼ë³€ ì˜ì—­
                eye_roi = gray[max(0, eye_y-15):eye_y+15, max(0, eye_x-15):eye_x+15]
                if eye_roi.size > 0:
                    bright_pixels = cv2.threshold(eye_roi, 200, 255, cv2.THRESH_BINARY)[1]
                    bright_ratio = np.sum(bright_pixels == 255) / eye_roi.size
                    
                    if bright_ratio > 0.1:  # 10% ì´ìƒ ë°ì€ í”½ì…€
                        reflection_count += 1
            
            if reflection_count >= 1:
                indicators += 1
            
            # ì•ˆê²½ ì°©ìš© íŒë‹¨
            confidence = indicators / 3.0
            wearing_glasses = confidence > 0.6
            
            return {
                'wearing_glasses': wearing_glasses,
                'confidence': round(confidence, 3),
                'indicators': indicators,
                'analysis': {
                    'nose_bridge_detected': indicators >= 1,
                    'frame_pattern_detected': indicators >= 2, 
                    'reflection_detected': reflection_count >= 1
                }
            }
            
        except Exception as e:
            return {
                'wearing_glasses': False,
                'confidence': 0.0,
                'error': str(e)
            }
    
    def detect_liveness(self, image: np.ndarray, landmarks: np.ndarray) -> Dict:
        """ìŠ¤í‘¸í•‘ ë°©ì§€ - ìƒì²´ ê°ì§€"""
        try:
            height, width = image.shape[:2]
            
            # 1. ì›€ì§ì„ ê°ì§€ (ì½”ë ì¶”ì )
            nose_tip = landmarks[1]  # ì½”ë
            nose_pos = (nose_tip.x * width, nose_tip.y * height)
            
            self.movement_history.append(nose_pos)
            if len(self.movement_history) > 10:
                self.movement_history.pop(0)
            
            movement_score = 0.5
            if len(self.movement_history) >= 3:
                movements = []
                for i in range(1, len(self.movement_history)):
                    prev_pos = self.movement_history[i-1]
                    curr_pos = self.movement_history[i]
                    movement = euclidean_distance(prev_pos, curr_pos)
                    movements.append(movement)
                
                avg_movement = np.mean(movements)
                
                # ìì—°ìŠ¤ëŸ¬ìš´ ì›€ì§ì„ ì ìˆ˜
                if 2 < avg_movement < 15:
                    movement_score = 1.0
                elif 1 < avg_movement < 25:
                    movement_score = 0.7
                else:
                    movement_score = 0.3
            
            # 2. í…ìŠ¤ì²˜ ë¶„ì„ (ì‹¤ì œ í”¼ë¶€ vs ì‚¬ì§„)
            face_center_x = int(landmarks[1].x * width)
            face_center_y = int(landmarks[1].y * height)
            
            # ì–¼êµ´ ì¤‘ì•™ ì˜ì—­ ì¶”ì¶œ
            roi_size = 40
            x1 = max(0, face_center_x - roi_size//2)
            y1 = max(0, face_center_y - roi_size//2)
            x2 = min(width, face_center_x + roi_size//2)
            y2 = min(height, face_center_y + roi_size//2)
            
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            face_roi = gray[y1:y2, x1:x2]
            
            texture_score = 0.5
            if face_roi.size > 0:
                # Laplacian ë¶„ì‚°ìœ¼ë¡œ í…ìŠ¤ì²˜ í’ˆì§ˆ ì¸¡ì •
                laplacian_var = cv2.Laplacian(face_roi, cv2.CV_64F).var()
                
                if 100 < laplacian_var < 800:  # ì‹¤ì œ í”¼ë¶€ ë²”ìœ„
                    texture_score = 1.0
                elif 50 < laplacian_var < 1200:
                    texture_score = 0.7
                else:
                    texture_score = 0.3
            
            # 3. ì–¼êµ´ í¬ê¸°/ë¹„ìœ¨ ê²€ì‚¬ (ê±°ë¦¬ ì¶”ì •)
            left_face = landmarks[234]  # ì™¼ìª½ ì–¼êµ´ ê²½ê³„
            right_face = landmarks[454]  # ì˜¤ë¥¸ìª½ ì–¼êµ´ ê²½ê³„
            
            face_width = abs(left_face.x - right_face.x) * width
            
            depth_score = 0.5
            if 60 < face_width < 250:  # ì ì ˆí•œ ê±°ë¦¬
                depth_score = 1.0
            elif 40 < face_width < 350:
                depth_score = 0.7
            else:
                depth_score = 0.3
            
            # ì¢…í•© ìƒì²´ ì ìˆ˜
            liveness_score = (movement_score * 0.4 + texture_score * 0.4 + depth_score * 0.2)
            is_real_person = liveness_score > 0.65
            
            return {
                'is_real_person': is_real_person,
                'liveness_score': round(liveness_score, 3),
                'movement_score': round(movement_score, 3),
                'texture_score': round(texture_score, 3),
                'depth_score': round(depth_score, 3),
                'face_width': round(face_width, 1),
                'anti_spoofing_passed': is_real_person
            }
            
        except Exception as e:
            return {
                'is_real_person': False,
                'liveness_score': 0.0,
                'error': str(e)
            }
    
    def process_image_advanced(self, image: np.ndarray) -> Dict:
        """ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        if not self.mediapipe_enabled:
            # MediaPipeê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ê²€ì¶œë§Œ
            return self.detect_faces_basic(image)
        
        try:
            # MediaPipeë¡œ ì–¼êµ´ ê²€ì¶œ ë° ëœë“œë§ˆí¬ ì¶”ì¶œ
            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            mesh_results = self.face_mesh.process(rgb_image)
            detection_results = self.face_detection.process(rgb_image)
            
            if not mesh_results.multi_face_landmarks:
                # MediaPipe ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ì¶œ ì‚¬ìš©
                return self.detect_faces_basic(image)
            
            # ì²« ë²ˆì§¸ ì–¼êµ´ ì²˜ë¦¬
            face_landmarks = mesh_results.multi_face_landmarks[0]
            landmarks = face_landmarks.landmark
            
            # ê° ë³´ì•ˆ ê¸°ëŠ¥ ì‹¤í–‰
            blink_result = self.detect_blink(landmarks, image.shape)
            glasses_result = self.detect_glasses(landmarks, image)
            liveness_result = self.detect_liveness(image, landmarks)
            
            # ì–¼êµ´ ê²½ê³„ ìƒì ê³„ì‚° (ê¸°ë³¸ í˜¸í™˜ì„± ìœ„í•´)
            face_bbox = self._calculate_face_bbox(landmarks, image.shape)
            
            # ì¢…í•© ë³´ì•ˆ ì ìˆ˜
            security_score = (
                (1.0 if blink_result['is_alive'] else 0.0) * 0.3 +
                liveness_result['liveness_score'] * 0.5 +
                (0.9 if not glasses_result['wearing_glasses'] else 0.7) * 0.2
            )
            
            return {
                'success': True,
                'face_count': len(mesh_results.multi_face_landmarks),
                'faces': [face_bbox] if face_bbox else [],
                'method': 'MediaPipe_Advanced',
                'security_features': {
                    'blink_detection': blink_result,
                    'glasses_detection': glasses_result,
                    'liveness_detection': liveness_result,
                    'security_score': round(security_score, 3),
                    'security_level': 'HIGH' if security_score > 0.8 else 'MEDIUM' if security_score > 0.6 else 'LOW',
                    'security_passed': security_score > 0.7
                },
                'image_size': {
                    'width': image.shape[1],
                    'height': image.shape[0]
                },
                'advanced_mode': True
            }
            
        except Exception as e:
            # ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ëª¨ë“œë¡œ í´ë°±
            print(f"ê³ ê¸‰ ëª¨ë“œ ì˜¤ë¥˜, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {e}")
            basic_result = self.detect_faces_basic(image)
            basic_result['fallback_to_basic'] = True
            return basic_result
    
    def _calculate_face_bbox(self, landmarks: np.ndarray, image_shape: Tuple[int, int]) -> Optional[Dict]:
        """MediaPipe ëœë“œë§ˆí¬ì—ì„œ ì–¼êµ´ ê²½ê³„ìƒì ê³„ì‚°"""
        try:
            height, width = image_shape[:2]
            
            # ì–¼êµ´ ê²½ê³„ ëœë“œë§ˆí¬ë“¤
            x_coords = [landmarks[i].x * width for i in [10, 234, 454, 152]]  # ì£¼ìš” ê²½ê³„ì ë“¤
            y_coords = [landmarks[i].y * height for i in [10, 234, 454, 152]]
            
            x_min, x_max = int(min(x_coords)), int(max(x_coords))
            y_min, y_max = int(min(y_coords)), int(max(y_coords))
            
            # ì—¬ë°± ì¶”ê°€
            margin = 20
            x_min = max(0, x_min - margin)
            y_min = max(0, y_min - margin)
            x_max = min(width, x_max + margin)
            y_max = min(height, y_max + margin)
            
            return {
                'x': x_min,
                'y': y_min,
                'width': x_max - x_min,
                'height': y_max - y_min,
                'confidence': 0.95
            }
        except:
            return None

    def simulate_recognition(self, face_data):
        """ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜ (í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            self.stats['total_requests'] += 1
            
            # ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜ ë¡œì§
            # ì‹¤ì œë¡œëŠ” face_recognition ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©
            
            # ëœë¤í•˜ê²Œ í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì ì¸ì‹
            if random.random() > 0.3:  # 70% í™•ë¥ ë¡œ ì„±ê³µ
                member_id = 999  # í…ŒìŠ¤íŠ¸ ì‚¬ìš©ì
                confidence = 0.75 + random.random() * 0.2  # 0.75-0.95
                
                self.stats['successful_recognitions'] += 1
                
                return {
                    'success': True,
                    'member_id': member_id,
                    'member_name': self.test_faces.get(member_id, "Unknown"),
                    'confidence': round(confidence, 4),
                    'simulation': True
                }
            else:
                return {
                    'success': False,
                    'error': 'No matching face found',
                    'simulation': True
                }
            
        except Exception as e:
            logger.error(f"ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e)}
    
    def check_database_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸"""
        try:
            conn = mysql.connector.connect(**self.db_config)
            cursor = conn.cursor()
            
            # member_faces í…Œì´ë¸” í™•ì¸
            cursor.execute("SHOW TABLES LIKE 'member_faces'")
            table_exists = cursor.fetchone() is not None
            
            if table_exists:
                cursor.execute("SELECT COUNT(*) FROM member_faces WHERE is_active = 1")
                face_count = cursor.fetchone()[0]
            else:
                face_count = 0
            
            cursor.close()
            conn.close()
            
            return {
                'connected': True,
                'table_exists': table_exists,
                'face_count': face_count
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}")
            return {
                'connected': False,
                'error': str(e)
            }

# Flask ì•± ìƒì„±
app = Flask(__name__)
CORS(app)
app.config['MAX_CONTENT_LENGTH'] = 16 * 1024 * 1024  # 16MB ì œí•œ

# ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
try:
    face_service = AdvancedFaceRecognizer()
    logger.info("ğŸ‰ ì„œë¹„ìŠ¤ ì¤€ë¹„ ì™„ë£Œ")
except Exception as e:
    logger.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    sys.exit(1)

@app.route('/api/face/health', methods=['GET'])
def health_check():
    """í—¬ìŠ¤ì²´í¬"""
    try:
        uptime = time.time() - face_service.stats['start_time']
        db_status = face_service.check_database_connection()
        
        return jsonify({
            "status": "healthy",
            "timestamp": time.time(),
            "uptime_seconds": int(uptime),
            "version": "1.0.0-advanced",
            "opencv_version": cv2.__version__,
            "stats": face_service.stats,
            "database": db_status,
            "features": {
                "face_detection": True,
                "face_recognition": False,  # ì•„ì§ êµ¬í˜„ ì•ˆë¨
                "simulation_mode": True
            }
        })
        
    except Exception as e:
        logger.error(f"í—¬ìŠ¤ì²´í¬ ì˜¤ë¥˜: {e}")
        return jsonify({
            "status": "unhealthy",
            "error": str(e),
            "timestamp": time.time()
        }), 500

@app.route('/api/face/detect', methods=['POST'])
def detect_faces():
    """ì–¼êµ´ ê²€ì¶œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        if 'image' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No image file provided'
            }), 400
        
        image_file = request.files['image']
        if image_file.filename == '':
            return jsonify({
                'success': False,
                'error': 'No image file selected'
            }), 400
        
        # ì´ë¯¸ì§€ ë°ì´í„° ì½ê¸°
        image_data = image_file.read()
        
        # ì–¼êµ´ ê²€ì¶œ
        result = face_service.process_image_advanced(cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR))
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"ì–¼êµ´ ê²€ì¶œ API ì˜¤ë¥˜: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/face/recognize_simulate', methods=['POST'])
def recognize_simulate():
    """ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        if 'image' not in request.files:
            return jsonify({
                'success': False,
                'error': 'No image file provided'
            }), 400
        
        image_file = request.files['image']
        if image_file.filename == '':
            return jsonify({
                'success': False,
                'error': 'No image file selected'
            }), 400
        
        # ì´ë¯¸ì§€ ë°ì´í„° ì½ê¸°
        image_data = image_file.read()
        
        # ë¨¼ì € ì–¼êµ´ ê²€ì¶œ
        detection_result = face_service.process_image_advanced(cv2.imdecode(np.frombuffer(image_data, np.uint8), cv2.IMREAD_COLOR))
        
        if not detection_result.get('success'):
            return jsonify({
                'success': False,
                'error': 'Face detection failed',
                'detection_error': detection_result.get('error')
            }), 400
        
        # ì‹œë®¬ë ˆì´ì…˜ ì‹¤í–‰
        result = face_service.simulate_recognition(detection_result)
        
        # ì²˜ë¦¬ ì‹œê°„ ì¶”ê°€
        result['processing_time_ms'] = 50 + int(np.random.random() * 100)  # 50-150ms
        result['faces_detected'] = detection_result.get('face_count', 0)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"ì¸ì‹ ì‹œë®¬ë ˆì´ì…˜ API ì˜¤ë¥˜: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/api/face/stats', methods=['GET'])
def get_stats():
    """í†µê³„ ì¡°íšŒ"""
    try:
        uptime = time.time() - face_service.stats['start_time']
        
        success_rate = 0
        if face_service.stats['total_requests'] > 0:
            success_rate = (face_service.stats['successful_recognitions'] / 
                           face_service.stats['total_requests'] * 100)
        
        return jsonify({
            'uptime_seconds': int(uptime),
            'stats': face_service.stats,
            'success_rate_percent': round(success_rate, 2),
            'database': face_service.check_database_connection()
        })
        
    except Exception as e:
        logger.error(f"í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/checkin', methods=['GET'])
def checkin_interface():
    """ì–¼êµ´ ì¸ì‹ ì²´í¬ì¸ ì›¹ ì¸í„°í˜ì´ìŠ¤"""
    return """<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SpoqPlus ê³ ê¸‰ ë³´ì•ˆ ì–¼êµ´ ì¸ì‹ ì²´í¬ì¸</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            color: #333;
        }
        .container {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
            text-align: center;
            max-width: 800px;
            width: 95%;
        }
        .header h1 {
            color: #2c3e50;
            font-size: 2.2em;
            margin-bottom: 10px;
            background: linear-gradient(45deg, #667eea, #764ba2);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .header p {
            color: #7f8c8d;
            font-size: 1em;
        }
        .video-container {
            position: relative;
            margin: 20px 0;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
        }
        #videoElement {
            width: 100%;
            height: 350px;
            object-fit: cover;
            display: block;
        }
        .face-overlay {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
        }
        .face-box {
            position: absolute;
            border: 3px solid #00ff00;
            border-radius: 5px;
            background: rgba(0, 255, 0, 0.1);
        }
        .face-label {
            position: absolute;
            background: rgba(0, 255, 0, 0.8);
            color: white;
            padding: 3px 8px;
            border-radius: 3px;
            font-size: 10px;
            font-weight: bold;
        }
        .btn {
            padding: 12px 25px;
            margin: 8px;
            border: none;
            border-radius: 8px;
            font-size: 1em;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        .btn-primary {
            background: linear-gradient(45deg, #667eea, #764ba2);
            color: white;
        }
        .btn-success {
            background: linear-gradient(45deg, #00b09b, #96c93d);
            color: white;
        }
        .btn:disabled {
            opacity: 0.6;
            cursor: not-allowed;
        }
        .status {
            margin: 15px 0;
            padding: 12px;
            border-radius: 8px;
            font-weight: bold;
        }
        .status.info {
            background: rgba(52, 152, 219, 0.1);
            border: 2px solid #3498db;
            color: #2980b9;
        }
        .status.success {
            background: rgba(46, 204, 113, 0.1);
            border: 2px solid #2ecc71;
            color: #27ae60;
        }
        .status.error {
            background: rgba(231, 76, 60, 0.1);
            border: 2px solid #e74c3c;
            color: #c0392b;
        }
        .status.warning {
            background: rgba(241, 196, 15, 0.1);
            border: 2px solid #f1c40f;
            color: #d68910;
        }
        .stats-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
            gap: 12px;
            margin: 20px 0;
        }
        .stat-card {
            background: rgba(255, 255, 255, 0.9);
            padding: 12px;
            border-radius: 8px;
            border: 2px solid #ecf0f1;
        }
        .stat-number {
            font-size: 1.4em;
            font-weight: bold;
            color: #2c3e50;
        }
        .stat-label {
            color: #7f8c8d;
            font-size: 0.8em;
            margin-top: 3px;
        }
        .security-panel {
            background: rgba(255, 255, 255, 0.8);
            border-radius: 10px;
            padding: 15px;
            margin: 15px 0;
            border: 2px solid #ecf0f1;
        }
        .security-title {
            font-size: 1.1em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        .security-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(120px, 1fr));
            gap: 8px;
        }
        .security-item {
            text-align: center;
            padding: 8px;
            border-radius: 6px;
            background: rgba(255, 255, 255, 0.6);
        }
        .security-value {
            font-weight: bold;
            font-size: 1.1em;
        }
        .security-value.good { color: #27ae60; }
        .security-value.warning { color: #f39c12; }
        .security-value.danger { color: #e74c3c; }
        .hidden { display: none; }
        .mode-indicator {
            position: absolute;
            top: 10px;
            right: 10px;
            padding: 5px 10px;
            background: rgba(0, 0, 0, 0.7);
            color: white;
            border-radius: 5px;
            font-size: 0.8em;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ›¡ï¸ SpoqPlus ê³ ê¸‰ ë³´ì•ˆ ì–¼êµ´ ì¸ì‹</h1>
            <p>MediaPipe ê¸°ë°˜ ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ (ëˆˆ ê¹œë¹¡ì„, ì•ˆê²½ ê°ì§€, ìŠ¤í‘¸í•‘ ë°©ì§€)</p>
        </div>

        <div class="video-container">
            <video id="videoElement" autoplay muted playsinline></video>
            <div class="face-overlay" id="faceOverlay"></div>
            <div class="mode-indicator" id="modeIndicator">ê¸°ë³¸ ëª¨ë“œ</div>
        </div>

        <div class="controls">
            <button id="startBtn" class="btn btn-primary">ğŸ“· ì›¹ìº  ì‹œì‘</button>
            <button id="checkinBtn" class="btn btn-success hidden" disabled>âœ… ë³´ì•ˆ ì²´í¬ì¸</button>
        </div>

        <div id="status" class="status info">
            ğŸ“‹ ì›¹ìº ì„ ì‹œì‘í•˜ë ¤ë©´ 'ì›¹ìº  ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”
        </div>

        <div class="stats-grid">
            <div class="stat-card">
                <div id="faceCount" class="stat-number">0</div>
                <div class="stat-label">ê²€ì¶œëœ ì–¼êµ´</div>
            </div>
            <div class="stat-card">
                <div id="recognitionStatus" class="stat-number">-</div>
                <div class="stat-label">ì¸ì‹ ìƒíƒœ</div>
            </div>
            <div class="stat-card">
                <div id="securityScore" class="stat-number">-</div>
                <div class="stat-label">ë³´ì•ˆ ì ìˆ˜</div>
            </div>
            <div class="stat-card">
                <div id="securityLevel" class="stat-number">-</div>
                <div class="stat-label">ë³´ì•ˆ ë“±ê¸‰</div>
            </div>
        </div>

        <div class="security-panel" id="securityPanel" style="display: none;">
            <div class="security-title">ğŸ” ì‹¤ì‹œê°„ ë³´ì•ˆ ë¶„ì„</div>
            <div class="security-grid">
                <div class="security-item">
                    <div class="security-label">ğŸ‘ï¸ ëˆˆ ê¹œë¹¡ì„</div>
                    <div id="blinkStatus" class="security-value">-</div>
                    <div id="blinkCount" style="font-size: 0.8em; color: #666;">0íšŒ</div>
                </div>
                <div class="security-item">
                    <div class="security-label">ğŸ‘“ ì•ˆê²½ ê°ì§€</div>
                    <div id="glassesStatus" class="security-value">-</div>
                    <div id="glassesConf" style="font-size: 0.8em; color: #666;">0%</div>
                </div>
                <div class="security-item">
                    <div class="security-label">ğŸ›¡ï¸ ìƒì²´ ê°ì§€</div>
                    <div id="livenessStatus" class="security-value">-</div>
                    <div id="livenessScore" style="font-size: 0.8em; color: #666;">0%</div>
                </div>
                <div class="security-item">
                    <div class="security-label">ğŸ¯ ì›€ì§ì„</div>
                    <div id="movementStatus" class="security-value">-</div>
                    <div id="movementScore" style="font-size: 0.8em; color: #666;">0%</div>
                </div>
            </div>
        </div>
    </div>

    <script>
        class AdvancedFaceRecognitionApp {
            constructor() {
                this.video = document.getElementById('videoElement');
                this.startBtn = document.getElementById('startBtn');
                this.checkinBtn = document.getElementById('checkinBtn');
                this.status = document.getElementById('status');
                this.faceOverlay = document.getElementById('faceOverlay');
                this.securityPanel = document.getElementById('securityPanel');
                this.modeIndicator = document.getElementById('modeIndicator');
                
                this.isStreaming = false;
                this.isProcessing = false;
                this.detectionInterval = null;
                this.lastRecognition = null;
                this.advancedMode = false;
                
                this.init();
            }

            init() {
                this.startBtn.addEventListener('click', () => this.startWebcam());
                this.checkinBtn.addEventListener('click', () => this.performCheckin());
                this.checkServerStatus();
            }

            async checkServerStatus() {
                try {
                    const response = await fetch('/api/face/health');
                    if (response.ok) {
                        const data = await response.json();
                        this.updateStatus('success', `ğŸŸ¢ ì„œë²„ ì—°ê²°ë¨ (OpenCV ${data.opencv_version})`);
                        
                        // ë³´ì•ˆ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸
                        const secResponse = await fetch('/api/face/security_status');
                        if (secResponse.ok) {
                            const secData = await secResponse.json();
                            this.advancedMode = secData.mediapipe_available;
                            this.modeIndicator.textContent = this.advancedMode ? 'ê³ ê¸‰ ë³´ì•ˆ ëª¨ë“œ' : 'ê¸°ë³¸ ëª¨ë“œ';
                            this.modeIndicator.style.background = this.advancedMode ? 'rgba(0, 255, 0, 0.8)' : 'rgba(255, 165, 0, 0.8)';
                        }
                    } else {
                        this.updateStatus('error', 'âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨');
                    }
                } catch (error) {
                    this.updateStatus('error', 'âŒ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤');
                }
            }

            async startWebcam() {
                try {
                    this.updateStatus('info', 'ğŸ“· ì›¹ìº  ì ‘ê·¼ ê¶Œí•œì„ ìš”ì²­ì¤‘...');
                    
                    const stream = await navigator.mediaDevices.getUserMedia({
                        video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' },
                        audio: false
                    });

                    this.video.srcObject = stream;
                    this.isStreaming = true;
                    
                    this.video.onloadedmetadata = () => {
                        this.updateStatus('success', 'âœ… ì›¹ìº ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì–¼êµ´ì„ ì¹´ë©”ë¼ì— ë§ì¶°ì£¼ì„¸ìš”');
                        this.startBtn.textContent = 'ğŸ›‘ ì›¹ìº  ì •ì§€';
                        this.startBtn.onclick = () => this.stopWebcam();
                        this.checkinBtn.classList.remove('hidden');
                        
                        if (this.advancedMode) {
                            this.securityPanel.style.display = 'block';
                        }
                        
                        this.startFaceDetection();
                    };

                } catch (error) {
                    this.updateStatus('error', 'âŒ ì›¹ìº ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”');
                }
            }

            stopWebcam() {
                if (this.video.srcObject) {
                    this.video.srcObject.getTracks().forEach(track => track.stop());
                    this.video.srcObject = null;
                }
                
                if (this.detectionInterval) {
                    clearInterval(this.detectionInterval);
                    this.detectionInterval = null;
                }
                
                this.isStreaming = false;
                this.startBtn.textContent = 'ğŸ“· ì›¹ìº  ì‹œì‘';
                this.startBtn.onclick = () => this.startWebcam();
                this.checkinBtn.classList.add('hidden');
                this.faceOverlay.innerHTML = '';
                this.securityPanel.style.display = 'none';
                
                this.updateStatus('info', 'ğŸ“‹ ì›¹ìº ì´ ì •ì§€ë˜ì—ˆìŠµë‹ˆë‹¤');
                this.resetStats();
            }

            startFaceDetection() {
                this.detectionInterval = setInterval(() => {
                    if (!this.isProcessing && this.isStreaming) {
                        this.detectFaces();
                    }
                }, 1000);
            }

            async detectFaces() {
                try {
                    this.isProcessing = true;
                    
                    const canvas = document.createElement('canvas');
                    canvas.width = this.video.videoWidth;
                    canvas.height = this.video.videoHeight;
                    const ctx = canvas.getContext('2d');
                    ctx.drawImage(this.video, 0, 0);
                    
                    const blob = await new Promise(resolve => canvas.toBlob(resolve, 'image/jpeg', 0.8));
                    
                    const formData = new FormData();
                    formData.append('image', blob, 'frame.jpg');
                    
                    // ê³ ê¸‰ ëª¨ë“œ ì‚¬ìš© ê°€ëŠ¥ ì‹œ ê³ ê¸‰ ê²€ì¶œ ì‚¬ìš©
                    const endpoint = this.advancedMode ? '/api/face/advanced_detect' : '/api/face/detect';
                    const response = await fetch(endpoint, {
                        method: 'POST',
                        body: formData
                    });
                    
                    if (response.ok) {
                        const data = await response.json();
                        this.handleDetectionResult(data);
                        
                        if (data.face_count > 0) {
                            this.attemptRecognition(formData);
                        }
                    }
                    
                } catch (error) {
                    console.error('ì–¼êµ´ ê²€ì¶œ ì˜¤ë¥˜:', error);
                } finally {
                    this.isProcessing = false;
                }
            }

            async attemptRecognition(formData) {
                try {
                    const response = await fetch('/api/face/recognize_simulate', {
                        method: 'POST',
                        body: formData
                    });
                    
                    if (response.ok) {
                        const data = await response.json();
                        this.handleRecognitionResult(data);
                    }
                } catch (error) {
                    console.error('ì¸ì‹ ì˜¤ë¥˜:', error);
                }
            }

            handleDetectionResult(data) {
                this.faceOverlay.innerHTML = '';
                
                // ì–¼êµ´ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
                if (data.success && data.faces) {
                    const videoRect = this.video.getBoundingClientRect();
                    const scaleX = videoRect.width / data.image_size.width;
                    const scaleY = videoRect.height / data.image_size.height;
                    
                    data.faces.forEach((face, index) => {
                        const faceBox = document.createElement('div');
                        faceBox.className = 'face-box';
                        faceBox.style.left = (face.x * scaleX) + 'px';
                        faceBox.style.top = (face.y * scaleY) + 'px';
                        faceBox.style.width = (face.width * scaleX) + 'px';
                        faceBox.style.height = (face.height * scaleY) + 'px';
                        
                        const label = document.createElement('div');
                        label.className = 'face-label';
                        label.textContent = `ì–¼êµ´ ${index + 1}`;
                        label.style.left = (face.x * scaleX) + 'px';
                        label.style.top = ((face.y * scaleY) - 20) + 'px';
                        
                        this.faceOverlay.appendChild(faceBox);
                        this.faceOverlay.appendChild(label);
                    });
                }
                
                // ê¸°ë³¸ í†µê³„ ì—…ë°ì´íŠ¸
                this.updateBasicStats(data.face_count, '-', '-', '-');
                
                // ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ ê²°ê³¼ ì²˜ë¦¬
                if (data.security_features) {
                    this.updateSecurityFeatures(data.security_features);
                }
                
                if (data.face_count > 0) {
                    const securityPassed = data.security_features ? data.security_features.security_passed : true;
                    if (securityPassed) {
                        this.updateStatus('success', `ğŸ‘ï¸ ${data.face_count}ê°œ ì–¼êµ´ ê²€ì¶œ - ë³´ì•ˆ ê²€ì¦ í†µê³¼`);
                        this.checkinBtn.disabled = false;
                    } else {
                        this.updateStatus('warning', `âš ï¸ ì–¼êµ´ ê²€ì¶œë˜ì—ˆìœ¼ë‚˜ ë³´ì•ˆ ê²€ì¦ ì‹¤íŒ¨`);
                        this.checkinBtn.disabled = true;
                    }
                } else {
                    this.updateStatus('warning', 'ğŸ‘€ ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ ì •ë©´ì„ ë´ì£¼ì„¸ìš”');
                    this.checkinBtn.disabled = true;
                }
            }

            updateSecurityFeatures(security) {
                if (!security) return;
                
                // ë³´ì•ˆ ì ìˆ˜ ë° ë“±ê¸‰
                document.getElementById('securityScore').textContent = (security.security_score * 100).toFixed(0) + '%';
                document.getElementById('securityLevel').textContent = security.security_level || '-';
                
                // ëˆˆ ê¹œë¹¡ì„
                if (security.blink_detection) {
                    const blink = security.blink_detection;
                    document.getElementById('blinkStatus').textContent = blink.is_alive ? 'ì •ìƒ' : 'ì˜ì‹¬';
                    document.getElementById('blinkStatus').className = 'security-value ' + (blink.is_alive ? 'good' : 'danger');
                    document.getElementById('blinkCount').textContent = blink.total_blinks + 'íšŒ';
                }
                
                // ì•ˆê²½ ê°ì§€
                if (security.glasses_detection) {
                    const glasses = security.glasses_detection;
                    document.getElementById('glassesStatus').textContent = glasses.wearing_glasses ? 'ì°©ìš©' : 'ë¯¸ì°©ìš©';
                    document.getElementById('glassesStatus').className = 'security-value good';
                    document.getElementById('glassesConf').textContent = (glasses.confidence * 100).toFixed(0) + '%';
                }
                
                // ìƒì²´ ê°ì§€
                if (security.liveness_detection) {
                    const liveness = security.liveness_detection;
                    document.getElementById('livenessStatus').textContent = liveness.is_real_person ? 'ì‹¤ì œ' : 'ê°€ì§œ';
                    document.getElementById('livenessStatus').className = 'security-value ' + (liveness.is_real_person ? 'good' : 'danger');
                    document.getElementById('livenessScore').textContent = (liveness.liveness_score * 100).toFixed(0) + '%';
                    document.getElementById('movementScore').textContent = (liveness.movement_score * 100).toFixed(0) + '%';
                }
            }

            handleRecognitionResult(data) {
                if (data.success) {
                    this.lastRecognition = data;
                    this.updateBasicStats(
                        document.getElementById('faceCount').textContent,
                        data.member_name,
                        document.getElementById('securityScore').textContent,
                        document.getElementById('securityLevel').textContent
                    );
                } else {
                    this.updateBasicStats(
                        document.getElementById('faceCount').textContent,
                        'ë¯¸ì¸ì‹',
                        document.getElementById('securityScore').textContent,
                        document.getElementById('securityLevel').textContent
                    );
                }
            }

            async performCheckin() {
                if (!this.lastRecognition) {
                    this.updateStatus('warning', 'âš ï¸ ë¨¼ì € ì–¼êµ´ ì¸ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤');
                    return;
                }
                
                this.checkinBtn.textContent = 'ì²´í¬ì¸ ì¤‘...';
                this.checkinBtn.disabled = true;
                
                try {
                    await new Promise(resolve => setTimeout(resolve, 1500));
                    
                    this.updateStatus('success', 
                        `ğŸ‰ ${this.lastRecognition.member_name}ë‹˜ ë³´ì•ˆ ì²´í¬ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!`
                    );
                    
                    setTimeout(() => {
                        this.checkinBtn.textContent = 'âœ… ë³´ì•ˆ ì²´í¬ì¸';
                        this.checkinBtn.disabled = false;
                        this.lastRecognition = null;
                    }, 3000);
                    
                } catch (error) {
                    this.updateStatus('error', 'âŒ ì²´í¬ì¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                    this.checkinBtn.textContent = 'âœ… ë³´ì•ˆ ì²´í¬ì¸';
                    this.checkinBtn.disabled = false;
                }
            }

            updateStatus(type, message) {
                this.status.className = `status ${type}`;
                this.status.textContent = message;
            }

            updateBasicStats(faceCount, recognition, securityScore, securityLevel) {
                document.getElementById('faceCount').textContent = faceCount;
                document.getElementById('recognitionStatus').textContent = recognition;
                if (securityScore !== undefined) document.getElementById('securityScore').textContent = securityScore;
                if (securityLevel !== undefined) document.getElementById('securityLevel').textContent = securityLevel;
            }

            resetStats() {
                this.updateBasicStats(0, '-', '-', '-');
                ['blinkStatus', 'glassesStatus', 'livenessStatus', 'movementStatus'].forEach(id => {
                    document.getElementById(id).textContent = '-';
                    document.getElementById(id).className = 'security-value';
                });
            }
        }

        document.addEventListener('DOMContentLoaded', () => {
            new AdvancedFaceRecognitionApp();
        });
    </script>
</body>
</html>"""

@app.route('/api/face/advanced_detect', methods=['POST'])
def advanced_face_detect():
    """ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥ì´ í¬í•¨ëœ ì–¼êµ´ ê²€ì¶œ"""
    try:
        if 'image' not in request.files:
            return jsonify({'error': 'No image provided'}), 400
        
        file = request.files['image']
        if file.filename == '':
            return jsonify({'error': 'No image selected'}), 400
        
        # ì´ë¯¸ì§€ ì½ê¸°
        image_data = file.read()
        nparr = np.frombuffer(image_data, np.uint8)
        image = cv2.imdecode(nparr, cv2.IMREAD_COLOR)
        
        if image is None:
            return jsonify({'error': 'Invalid image format'}), 400
        
        # ê³ ê¸‰ ë³´ì•ˆ ì²˜ë¦¬ ì‹¤í–‰
        result = face_service.process_image_advanced(image)
        
        return jsonify(result)
        
    except Exception as e:
        logger.error(f"ê³ ê¸‰ ì–¼êµ´ ê²€ì¶œ ì˜¤ë¥˜: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/face/security_status', methods=['GET'])
def security_status():
    """ë³´ì•ˆ ê¸°ëŠ¥ ìƒíƒœ í™•ì¸"""
    try:
        return jsonify({
            'mediapipe_available': face_service.mediapipe_enabled,
            'security_features': {
                'blink_detection': face_service.mediapipe_enabled,
                'glasses_detection': face_service.mediapipe_enabled,
                'liveness_detection': face_service.mediapipe_enabled,
                'anti_spoofing': face_service.mediapipe_enabled
            },
            'current_stats': {
                'total_blinks': getattr(face_service, 'total_blinks', 0),
                'movement_history_size': len(getattr(face_service, 'movement_history', [])),
                'texture_scores_size': len(getattr(face_service, 'texture_scores', []))
            },
            'version': '2.0.0-advanced',
            'timestamp': time.time()
        })
    except Exception as e:
        return jsonify({'error': str(e)}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ Starting Advanced Face Recognition Service on localhost:5000")
    logger.info("ğŸ“ OpenCV + MediaPipe ê¸°ë°˜ ê³ ê¸‰ ë³´ì•ˆ ê¸°ëŠ¥")
    app.run(host='0.0.0.0', port=5000, debug=True, threaded=True) 